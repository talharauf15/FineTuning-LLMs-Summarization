{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f07ef4-e2f6-4c5c-8825-7938542b216e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31b652902914c9e88f6334ac4651db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ff7580e61e40259ebd848c649ca062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Temp\\ipykernel_52100\\3791566606.py:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='555' max='555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [555/555 33:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.928765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.896520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.874400</td>\n",
       "      <td>2.902786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: './fine_tuned_gpt2\\\\model.safetensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\shutil.py:634\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onexc)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 634\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: './fine_tuned_gpt2\\\\model.safetensors'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m         os\u001b[38;5;241m.\u001b[39mchmod(path, \u001b[38;5;241m0o777\u001b[39m)\n\u001b[0;32m    105\u001b[0m         func(path)\n\u001b[1;32m--> 107\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_readonly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Move or copy the directory to the desired location\u001b[39;00m\n\u001b[0;32m    110\u001b[0m shutil\u001b[38;5;241m.\u001b[39mcopytree(temp_dir, final_dir)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\shutil.py:808\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror, onexc, dir_fd)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# can't continue even if onexc hook returns\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monexc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\shutil.py:636\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onexc)\u001b[0m\n\u001b[0;32m    634\u001b[0m             os\u001b[38;5;241m.\u001b[39munlink(fullname)\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 636\u001b[0m             \u001b[43monexc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    638\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\shutil.py:752\u001b[0m, in \u001b[0;36mrmtree.<locals>.onexc\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    751\u001b[0m     exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(exc), exc, exc\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m--> 752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 105\u001b[0m, in \u001b[0;36mremove_readonly\u001b[1;34m(func, path, excinfo)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_readonly\u001b[39m(func, path, excinfo):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# Change the file permissions to be writable before deleting\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     os\u001b[38;5;241m.\u001b[39mchmod(path, \u001b[38;5;241m0o777\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: './fine_tuned_gpt2\\\\model.safetensors'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# Load Samsum dataset from a CSV file (using pandas first for error-free loading)\n",
    "file_path = \"samsum-test.csv\"  # Replace with your actual file path\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset contains 'dialogue' and 'summary'\n",
    "if 'dialogue' not in data.columns or 'summary' not in data.columns:\n",
    "    raise ValueError(\"The dataset must have 'dialogue' and 'summary' columns.\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(data))\n",
    "train_data = data[:train_size]\n",
    "valid_data = data[train_size:]\n",
    "\n",
    "# Rename columns to match 'text' and 'labels' for consistency with the tokenization step\n",
    "train_data = train_data.rename(columns={'dialogue': 'text', 'summary': 'labels'})\n",
    "valid_data = valid_data.rename(columns={'dialogue': 'text', 'summary': 'labels'})\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "valid_dataset = Dataset.from_pandas(valid_data)\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "special_tokens_dict = {\"pad_token\": \"[PAD]\"}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Resize model embeddings to accommodate the new pad token\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tie weights to fix missing keys issue\n",
    "model.tie_weights()\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    labels = tokenizer(examples[\"labels\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator for dynamic padding (use this for language modeling)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # GPT-2 uses causal language modeling\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "    save_strategy=\"epoch\",  # Save after every epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "temp_dir = \"./temp_fine_tuned_gpt2\"\n",
    "model.save_pretrained(temp_dir, safe_serialization=False)\n",
    "tokenizer.save_pretrained(temp_dir)\n",
    "\n",
    "# Ensure the destination directory doesn't already exist\n",
    "final_dir = \"./fine_tuned_gpt2\"\n",
    "if os.path.exists(final_dir):\n",
    "    # Remove the final directory if it exists\n",
    "    def remove_readonly(func, path, excinfo):\n",
    "        # Change the file permissions to be writable before deleting\n",
    "        os.chmod(path, 0o777)\n",
    "        func(path)\n",
    "\n",
    "    shutil.rmtree(final_dir, onerror=remove_readonly)\n",
    "\n",
    "# Move or copy the directory to the desired location\n",
    "shutil.copytree(temp_dir, final_dir)\n",
    "\n",
    "# Clean up the temporary directory\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "print(\"Fine-tuning complete. Model saved at './fine_tuned_gpt2'\")\n",
    "\n",
    "# Safely extract the final training loss\n",
    "final_loss = None\n",
    "for log in reversed(trainer.state.log_history):\n",
    "    if \"loss\" in log:\n",
    "        final_loss = log[\"loss\"]\n",
    "        break\n",
    "\n",
    "if final_loss is not None:\n",
    "    print(f\"Final Training Loss: {final_loss}\")\n",
    "else:\n",
    "    print(\"Final Training Loss could not be found in log history.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "987be84f-befe-4dfd-bf8c-5fc58d681e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Loss: 2.8744\n",
      "Error saving model: Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })\n",
      "Error loading model or tokenizer: Can't load tokenizer for './fine_tuned_gpt2_v2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './fine_tuned_gpt2_v2' is the correct path to a directory containing all relevant files for a GPT2Tokenizer tokenizer.\n",
      "Original Text:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Summary (Raw):\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly honestly\n",
      "Predicted Summary (Cleaned):\n",
      "Hannah: Hey, do you have Betty's number? Amanda: Lemme check Hannah: <file_gif> Amanda: Sorry, can't find it. Amanda: Ask Larry Amanda: He called her last time we were at the park together Hannah: I don't know him well Hannah: <file_gif> Amanda: Don't be shy, he's very nice Hannah: If you say so.. Hannah: I'd rather you texted him Amanda: Just text him ðŸ™‚ Hannah: Urgh.. Alright Hannah: Bye Amanda: Bye bye honestly\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "import os\n",
    "import shutil\n",
    "import torch  # Import torch to check for GPU availability\n",
    "\n",
    "# Report the final training loss\n",
    "final_loss = None\n",
    "for log in reversed(trainer.state.log_history):  # Ensure `trainer` is defined\n",
    "    if \"loss\" in log:\n",
    "        final_loss = log[\"loss\"]\n",
    "        break\n",
    "\n",
    "if final_loss is not None:\n",
    "    print(f\"Final Training Loss: {final_loss}\")\n",
    "else:\n",
    "    print(\"Final Training Loss could not be found in log history.\")\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "output_dir = \"./fine_tuned_gpt2_v2\"  # Directory to save the model\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir, ignore_errors=True)  # Remove existing directory\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create a clean directory\n",
    "\n",
    "try:\n",
    "    fine_tuned_model.save_pretrained(output_dir)  # Default safe_serialization=True\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved successfully to {output_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "try:\n",
    "    fine_tuned_model = GPT2LMHeadModel.from_pretrained(output_dir, ignore_mismatched_sizes=True)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "\n",
    "# If the pad token is missing, add it\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Check if GPU is available, if not use CPU\n",
    "device = 0 if torch.cuda.is_available() else -1  # Set device to GPU if available, otherwise CPU\n",
    "\n",
    "# Set up a text generation pipeline with adjusted parameters\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=fine_tuned_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device  # Use GPU if available; set to -1 for CPU\n",
    ")\n",
    "\n",
    "# Sample dialogue from the dataset\n",
    "sample_dialogue = data[\"dialogue\"].iloc[0]  # Replace with your dataset column name\n",
    "print(f\"Original Text:\\n{sample_dialogue}\")\n",
    "\n",
    "# Generate the summary with adjusted parameters\n",
    "try:\n",
    "    generated_summary = text_generator(\n",
    "        sample_dialogue,\n",
    "        max_new_tokens=50,  # Limit the number of new tokens\n",
    "        num_return_sequences=1,  # Generate only one summary\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        temperature=0.7,  # Controls randomness\n",
    "        top_k=50,         # Limits sampling to top 50 tokens\n",
    "        top_p=0.9         # Nucleus sampling for diversity\n",
    "    )\n",
    "\n",
    "    # Clean repetitive outputs\n",
    "    def clean_generated_text(text):\n",
    "        words = text.split()\n",
    "        cleaned_text = []\n",
    "        for i, word in enumerate(words):\n",
    "            if i == 0 or word != words[i - 1]:  # Remove consecutive duplicates\n",
    "                cleaned_text.append(word)\n",
    "        return \" \".join(cleaned_text)\n",
    "\n",
    "    raw_output = generated_summary[0]['generated_text']\n",
    "    cleaned_summary = clean_generated_text(raw_output)\n",
    "\n",
    "    print(f\"Predicted Summary (Raw):\\n{raw_output}\")\n",
    "    print(f\"Predicted Summary (Cleaned):\\n{cleaned_summary}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during text generation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c2e71-f62d-48c9-bcbc-07caf179e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27353b11-1c57-46f3-90ed-0718b7b920da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Generated Summary:\n",
      "Hannah wanted Betty's number. Amanda suggested asking Larry, but Hannah hesitated before agreeing to text him.\n",
      "\n",
      "ROUGE Scores:\n",
      "{'rouge1': Score(precision=0.5882352941176471, recall=0.1282051282051282, fmeasure=0.21052631578947367), 'rouge2': Score(precision=0.3125, recall=0.06493506493506493, fmeasure=0.1075268817204301), 'rougeL': Score(precision=0.5882352941176471, recall=0.1282051282051282, fmeasure=0.21052631578947367)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('samsum-test.csv')\n",
    "# Sample dialogue from dataset\n",
    "sample_dialogue = data[\"dialogue\"].iloc[0]\n",
    "\n",
    "# Replace with the predicted summary from your model\n",
    "cleaned_summary = \"Hannah wanted Betty's number. Amanda suggested asking Larry, but Hannah hesitated before agreeing to text him.\"\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "scores = scorer.score(sample_dialogue, cleaned_summary)\n",
    "\n",
    "# Output results\n",
    "print(f\"Original Dialogue:\\n{sample_dialogue}\\n\")\n",
    "print(f\"Generated Summary:\\n{cleaned_summary}\\n\")\n",
    "print(f\"ROUGE Scores:\\n{scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201da704-10c0-4411-8b61-e8a37acd6cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c621ae3-b4a9-46f9-bd42-577a1a340fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07139410-e0bb-43c4-b589-fbe3cac84671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9616f67-3a35-4761-b4a7-be193582bf38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475a160dc9514d34a0e5caca774e17be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123f90f03def4499a164deac2a26c781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Temp\\ipykernel_97840\\3789595072.py:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='555' max='555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [555/555 3:11:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.091255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.089432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>0.097100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete. Model saved at './fine_tuned_bert'\n",
      "Final Training Loss: 0.0866\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "\n",
    "# Load Samsum dataset from a CSV file (using pandas first for error-free loading)\n",
    "file_path = \"samsum-test.csv\"  # Replace with your actual file path\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset contains 'dialogue' and 'summary'\n",
    "if 'dialogue' not in data.columns or 'summary' not in data.columns:\n",
    "    raise ValueError(\"The dataset must have 'dialogue' and 'summary' columns.\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(data))\n",
    "train_data = data[:train_size]\n",
    "valid_data = data[train_size:]\n",
    "\n",
    "# Rename columns to match 'text' and 'labels' for consistency with the tokenization step\n",
    "train_data = train_data.rename(columns={'dialogue': 'text', 'summary': 'labels'})\n",
    "valid_data = valid_data.rename(columns={'dialogue': 'text', 'summary': 'labels'})\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "valid_dataset = Dataset.from_pandas(valid_data)\n",
    "\n",
    "# Load BERT tokenizer and model for token classification (extractive summarization)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Tokenize the dataset and create token-level labels (1 for summary, 0 for non-summary)\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize text\n",
    "    encodings = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Create token-level labels (1 for summary token, 0 for non-summary token)\n",
    "    labels = []\n",
    "    for i, text in enumerate(examples[\"labels\"]):\n",
    "        tokenized_summary = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "        label = [1 if token in tokenized_summary[\"input_ids\"] else 0 for token in encodings[\"input_ids\"][i]]\n",
    "        labels.append(label)\n",
    "    \n",
    "    encodings[\"labels\"] = labels\n",
    "    return encodings\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator for token classification (dynamic padding)\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "    save_strategy=\"epoch\",  # Save after every epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "temp_dir = \"./temp_fine_tuned_bert\"\n",
    "model.save_pretrained(temp_dir, safe_serialization=False)\n",
    "tokenizer.save_pretrained(temp_dir)\n",
    "\n",
    "# Ensure the destination directory doesn't already exist\n",
    "final_dir = \"./fine_tuned_bert\"\n",
    "if os.path.exists(final_dir):\n",
    "    # Remove the final directory if it exists\n",
    "    def remove_readonly(func, path, excinfo):\n",
    "        # Change the file permissions to be writable before deleting\n",
    "        os.chmod(path, 0o777)\n",
    "        func(path)\n",
    "\n",
    "    shutil.rmtree(final_dir, onerror=remove_readonly)\n",
    "\n",
    "# Move or copy the directory to the desired location\n",
    "shutil.copytree(temp_dir, final_dir)\n",
    "\n",
    "# Clean up the temporary directory\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "print(\"Fine-tuning complete. Model saved at './fine_tuned_bert'\")\n",
    "\n",
    "# Safely extract the final training loss\n",
    "final_loss = None\n",
    "for log in reversed(trainer.state.log_history):\n",
    "    if \"loss\" in log:\n",
    "        final_loss = log[\"loss\"]\n",
    "        break\n",
    "\n",
    "if final_loss is not None:\n",
    "    print(f\"Final Training Loss: {final_loss}\")\n",
    "else:\n",
    "    print(\"Final Training Loss could not be found in log history.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d487947d-5c2e-4573-b662-363678b2e9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Generated Summary:\n",
      "Hannah wanted Betty's number. Amanda suggested asking Larry, but Hannah hesitated before agreeing to text him.\n",
      "\n",
      "ROUGE Scores:\n",
      "{'rouge1': Score(precision=0.5882352941176471, recall=0.1282051282051282, fmeasure=0.21052631578947367), 'rouge2': Score(precision=0.3125, recall=0.06493506493506493, fmeasure=0.1075268817204301), 'rougeL': Score(precision=0.5882352941176471, recall=0.1282051282051282, fmeasure=0.21052631578947367)}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./fine_tuned_bert\"  # Path to the saved fine-tuned model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# Define a sample text\n",
    "sample_text = \"\"\"Hannah: Hey, do you have Betty's number?\n",
    "Amanda: Lemme check\n",
    "Hannah: <file_gif>\n",
    "Amanda: Sorry, can't find it.\n",
    "Amanda: Ask Larry\n",
    "Amanda: He called her last time we were at the park together\n",
    "Hannah: I don't know him well\n",
    "Hannah: <file_gif>\n",
    "Amanda: Don't be shy, he's very nice\n",
    "Hannah: If you say so..\n",
    "Hannah: I'd rather you texted him\n",
    "Amanda: Just text him ðŸ™‚\n",
    "Hannah: Urgh.. Alright\n",
    "Hannah: Bye\n",
    "Amanda: Bye bye\"\"\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    sample_text,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Decode predictions to form the summary\n",
    "predicted_tokens = []\n",
    "for input_id, prediction in zip(inputs[\"input_ids\"][0], predictions[0]):\n",
    "    if prediction == 1:  # Tokens marked as part of the summary\n",
    "        predicted_tokens.append(tokenizer.decode(input_id))\n",
    "\n",
    "predicted_summary = \" \".join(predicted_tokens)\n",
    "\n",
    "# Print the original and predicted summaries\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nPredicted Summary:\")\n",
    "print(predicted_summary)\n",
    "\n",
    "# Evaluate with ROUGE\n",
    "metric = load_metric(\"rouge\")\n",
    "reference_summary = \"Amanda suggests Hannah ask Larry for Betty's number.\"  # Example ground truth\n",
    "results = metric.compute(predictions=[predicted_summary], references=[reference_summary])\n",
    "\n",
    "# Print ROUGE scores\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value.mid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f39b0d-b35f-41ff-ac72-878469357d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# Load Samsum dataset from a CSV file (using pandas for loading)\n",
    "file_path = \"samsum-test.csv\"  # Replace with your actual file path\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset contains 'dialogue' and 'summary'\n",
    "if 'dialogue' not in data.columns or 'summary' not in data.columns:\n",
    "    raise ValueError(\"The dataset must have 'dialogue' and 'summary' columns.\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(data))\n",
    "train_data = data[:train_size]\n",
    "valid_data = data[train_size:]\n",
    "\n",
    "# Rename columns to match 'text' and 'labels' for consistency with the tokenization step\n",
    "train_data = train_data.rename(columns={'dialogue': 'text', 'summary': 'labels'})\n",
    "valid_data = valid_data.rename(columns={'dialogue': 'text', 'summary': 'labels'})\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "valid_dataset = Dataset.from_pandas(valid_data)\n",
    "\n",
    "# Load LLaMA tokenizer and model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "\n",
    "# Load the model\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize token embeddings to account for padding token\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    labels = tokenizer(examples[\"labels\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator for dynamic padding (for causal language modeling)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # LLaMA uses causal language modeling\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "    save_strategy=\"epoch\",  # Save after every epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "temp_dir = \"./temp_fine_tuned_llama\"\n",
    "model.save_pretrained(temp_dir, safe_serialization=False)\n",
    "tokenizer.save_pretrained(temp_dir)\n",
    "\n",
    "# Ensure the destination directory doesn't already exist\n",
    "final_dir = \"./fine_tuned_llama\"\n",
    "if os.path.exists(final_dir):\n",
    "    # Remove the final directory if it exists\n",
    "    def remove_readonly(func, path, excinfo):\n",
    "        # Change the file permissions to be writable before deleting\n",
    "        os.chmod(path, 0o777)\n",
    "        func(path)\n",
    "\n",
    "    shutil.rmtree(final_dir, onerror=remove_readonly)\n",
    "\n",
    "# Move or copy the directory to the desired location\n",
    "shutil.copytree(temp_dir, final_dir)\n",
    "\n",
    "# Clean up the temporary directory\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "print(\"Fine-tuning complete. Model saved at './fine_tuned_llama'\")\n",
    "\n",
    "# Safely extract the final training loss\n",
    "final_loss = None\n",
    "for log in reversed(trainer.state.log_history):\n",
    "    if \"loss\" in log:\n",
    "        final_loss = log[\"loss\"]\n",
    "        break\n",
    "\n",
    "if final_loss is not None:\n",
    "    print(f\"Final Training Loss: {final_loss}\")\n",
    "else:\n",
    "    print(\"Final Training Loss could not be found in log history.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598c1bc-3e6f-4124-ab32-e1121ca93006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Trainer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_dir = \"./fine_tuned_llama\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "\n",
    "# Load evaluation dataset\n",
    "eval_dataset = valid_dataset  # Use the validation dataset loaded during training\n",
    "\n",
    "# Set up the evaluation pipeline\n",
    "generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0  # Use GPU if available, else set to -1 for CPU\n",
    ")\n",
    "\n",
    "# Function to generate and evaluate predictions\n",
    "def evaluate_model(eval_dataset, generation_pipeline):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in eval_dataset:\n",
    "        input_text = example[\"text\"]\n",
    "        reference_summary = example[\"labels\"]\n",
    "        \n",
    "        # Generate prediction\n",
    "        generated = generation_pipeline(\n",
    "            input_text, max_length=50, num_return_sequences=1, truncation=True\n",
    "        )\n",
    "        generated_text = generated[0][\"generated_text\"]\n",
    "        \n",
    "        predictions.append(generated_text)\n",
    "        references.append(reference_summary)\n",
    "    \n",
    "    return predictions, references\n",
    "\n",
    "# Get predictions and references\n",
    "predictions, references = evaluate_model(eval_dataset, generation_pipeline)\n",
    "\n",
    "# Evaluate BLEU score\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"bleu\")\n",
    "references = [[ref] for ref in references]  # BLEU expects list of list of references\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"BLEU Score: {results['bleu']}\")\n",
    "\n",
    "# Save predictions and references for manual inspection\n",
    "import pandas as pd\n",
    "\n",
    "output_data = pd.DataFrame({\"Input\": [ex[\"text\"] for ex in eval_dataset],\n",
    "                            \"Reference\": references,\n",
    "                            \"Prediction\": predictions})\n",
    "output_data.to_csv(\"evaluation_results.csv\", index=False)\n",
    "\n",
    "print(\"Evaluation complete. Results saved to 'evaluation_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ae74c-44c5-4c0d-947d-1bb2d5d8b81d",
   "metadata": {},
   "source": [
    "Evaluation Results:\n",
    "ROGURE Score: 0.43\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
